{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75a229ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ac6c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn theta by taking num_iters gradient steps\n",
    "    with learning rate alpha.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        The feature matrix of shape (m, n+1) where m is the number of examples\n",
    "        and n is the number of features. The first column should be all ones.\n",
    "    y : array-like\n",
    "        The target values of shape (m,).\n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    num_iters : int\n",
    "        The number of iterations to run gradient descent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta : array-like\n",
    "        The learned linear regression parameters. A vector of shape (n+1,).\n",
    "    J_history : list\n",
    "        A list of the cost function values for each iteration of gradient descent.\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)  # initialize parameters to zero\n",
    "    J_history = []  # to keep track of the cost function values\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Compute the hypothesis (predicted values)\n",
    "        h = np.dot(X, theta)\n",
    "\n",
    "        # Compute the gradient of the cost function with respect to theta\n",
    "        #We then compute the gradient of the cost function with respect to theta. \n",
    "        #This is done using the formula for the gradient of the mean squared error (MSE) cost function\n",
    "        grad = (1/m) * np.dot(X.T, (h - y))\n",
    "\n",
    "        # Update the parameters\n",
    "        theta = theta - alpha * grad\n",
    "\n",
    "        # Compute the cost function J for the current parameters\n",
    "        # The cost function is defined as the mean squared error\n",
    "        J = 1/(2*m) * np.sum((h - y)**2)\n",
    "        J_history.append(J)\n",
    "    \n",
    "    #Finally, we return the learned parameters theta and the list of cost function values J_history.\n",
    "    return theta, J_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5444bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 2.],\n",
       "       [1., 1., 3.],\n",
       "       [1., 1., 4.],\n",
       "       [1., 1., 5.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n",
    "y = np.array([3, 4, 5, 6])\n",
    "\n",
    "# Add a column of ones to X\n",
    "# We add a column of ones to X because we want to incorporate the bias term (also known as the intercept term)\n",
    "# in our linear regression model.\n",
    "# np.c_ is used for horizontal concatenation \n",
    "\n",
    "\"\"\"In [1]: from numpy import c_\n",
    "In [2]: a = ones(4)\n",
    "In [3]: b = zeros((4,10))    \n",
    "In [4]: c_[a,b]\n",
    "Out[4]: \n",
    "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\"\"\"\n",
    "\n",
    "X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58d9d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, J_history = gradient_descent(X, y, alpha=0.1, num_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bce19fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:  [0.49999998 0.49999998 1.00000001]\n",
      "Last few values of J_history:  [5.0590067994971576e-17, 4.8954226890936755e-17, 4.737128076267675e-17, 4.583952215259846e-17, 4.435728835764239e-17]\n"
     ]
    }
   ],
   "source": [
    "print(\"theta: \", theta)\n",
    "print(\"Last few values of J_history: \", J_history[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6555136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value of y for [1, 6, 7]:  10.499999949400335\n"
     ]
    }
   ],
   "source": [
    "# Predict y for new data point [1, 6, 7]\n",
    "x_new = np.array([1, 6, 7])\n",
    "y_pred = np.dot(x_new, theta)\n",
    "print(\"Predicted value of y for [1, 6, 7]: \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91441320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
